{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "from scipy import stats as ss\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "plt.style.use( 'bmh' )\n",
    "plt.rcParams['figure.figsize'] = [25, 12]\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"/content/drive/MyDrive/dsp/\" for colab\n",
    "\n",
    "with open('code7_x_training.pkl', 'rb') as f:\n",
    "    x_training = pickle.load(f)\n",
    "with open('code7_x_train.pkl', 'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "with open('code7_x_test.pkl', 'rb') as f:\n",
    "    x_test = pickle.load(f)\n",
    "with open('code6_Y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('code6_Y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model_name,x_training,kfold,model,verbose=False):\n",
    "  mae_list=[]\n",
    "  mape_list=[]\n",
    "  rmse_list=[]\n",
    "\n",
    "  for k in range(kfold,0,-1):\n",
    "    if verbose:\n",
    "      print('\\nKfold number: {}'.format(k))\n",
    "    val_start_date=x_training['date'].max()- datetime.timedelta(days=k*6*7)\n",
    "    val_end_date=x_training['date'].max()- datetime.timedelta(days=(k-1)*6*7)\n",
    "\n",
    "    #filtering dataset\n",
    "    training = x_training[x_training['date'] < val_start_date]\n",
    "    validation = x_training[(x_training['date'] >= val_start_date) & (x_training['date'] >= val_end_date)]\n",
    "\n",
    "    # training and validation dataset\n",
    "    xtraining=training.drop(['date','sales'],axis=1)\n",
    "    ytraining=training['sales']\n",
    "    xvalidation=validation.drop(['date','sales'],axis=1)\n",
    "    yvalidation=validation['sales']\n",
    "\n",
    "    #model\n",
    "    m=model.fit(xtraining,ytraining)\n",
    "\n",
    "    #prediction\n",
    "    yhat=m.predict(xvalidation)\n",
    "\n",
    "    #performance\n",
    "    m_result=ml_error(model_name,np.expm1(yvalidation),np.expm1(yhat))\n",
    "\n",
    "    #store performance for each kfold iteration\n",
    "    mae_list.append(m_result['MAE'])\n",
    "    mape_list.append(m_result['MAPE'])\n",
    "    rmse_list.append(m_result['RMSE'])\n",
    "\n",
    "  return pd.DataFrame({ 'Model Name': model_name,\n",
    "                        'MAE CV':np.round(np.mean(mae_list),2).astype(str)+' +/- '+np.round(np.std(mae_list),2).astype(str),\n",
    "                        'MAPE CV':np.round(np.mean(mape_list),2).astype(str)+' +/- '+np.round(np.std(mape_list),2).astype(str),\n",
    "                        'RMSE CV':np.round(np.mean(rmse_list),2).astype(str)+' +/- '+np.round(np.std(rmse_list),2).astype(str),\n",
    "                        },index=[0])\n",
    "def mean_absolute_percentage_error(y,yhat):\n",
    "    return np.mean(np.abs((y-yhat)/y))\n",
    "\n",
    "def ml_error(model_name,y,yhat):\n",
    "    from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "    \n",
    "    mae=mean_absolute_error(y,yhat)\n",
    "    mape=mean_absolute_percentage_error(y,yhat)\n",
    "    rmse=np.sqrt(mean_squared_error(y,yhat))\n",
    "    \n",
    "    return pd.DataFrame({'Model name': model_name,\n",
    "                        'MAE': mae,\n",
    "                        'MAPE': mape,\n",
    "                        'RMSE': rmse}, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Hyperparameter fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos fazer o fine tuning para o modelo que selecionamos para seguir para produção. Na prática o Random Forest foi o modelo que performou melhor, porém pelo alto custo computacional, resolvemos escolher o XGBoost. Por essa razão, também aplicaremos o finetuning usando esse modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Random Search [NÃO RODAR LOCAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param={\n",
    "    'n_estimators':[1500,1700,2500,3000,3500],\n",
    "    'eta':[0.01,0.03],\n",
    "    'max_depth':[3,5,9],\n",
    "    'subsample':[0.1,0.5,0.7],\n",
    "    'colsample_bytree':[0.3,0.7,0.9],\n",
    "    'min_child_weight':[3,8,15]\n",
    "        }\n",
    "\n",
    "MAX_EVAL=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_resul=pd.DataFrame()\n",
    "\n",
    "for i in range(MAX_EVAL)\n",
    "    # choose values for parameters randomlu\n",
    "    hp = {k: random.sample(v,1)[0] for k,v in param.items()}\n",
    "    print(hp)\n",
    "    \n",
    "    #model\n",
    "    model_xgb=xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                               n_estimators=hp['n_estimators'],\n",
    "                               eta=hp['eta'],\n",
    "                               max_depth=hp['max_depth'],\n",
    "                               subsample=hp['subsample'],\n",
    "                               colsample_bytree=hp['colsample_bytree'],\n",
    "                               min_child_weight=hp['min_child_weight'])\n",
    "    \n",
    "    #performance\n",
    "    model_xgb_result=cross_validation('XGBoost Regressor',x_training,5,model_xgb,verbose=False)\n",
    "    final_result=pd.concat([final_result,model_xgb_result])\n",
    "    \n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inline-style: \n",
    "![alt text](/media/svncjus/vdsshd3/1_ds/__old/Study_DSP/dsemproducao/final_result.png\n",
    "\"final_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parametros mais otimizados de acordo com o resultado final (a foto está no repositório), \n",
    "# pois não processamos localmente as iterações do Random Search\n",
    "\n",
    "param_tuned={\n",
    "    'n_estimators':3000,\n",
    "    'eta':0.03,\n",
    "    'max_depth':5,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.7,\n",
    "    'min_child_weight':3\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.4.1-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "Requirement already satisfied: scipy in /home/svncjus/anaconda3/lib/python3.8/site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/svncjus/anaconda3/lib/python3.8/site-packages (from xgboost) (1.19.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "# run conda install xgboot on terminal\n",
    "import xgboost as xgb\n",
    "\n",
    "model_xgb_tuned=xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                           n_estimators=param_tuned['n_estimators'],\n",
    "                           eta=param_tuned['eta'],\n",
    "                           max_depth=param_tuned['max_depth'],\n",
    "                           subsample=param_tuned['subsample'],\n",
    "                           colsample_bytree=param_tuned['colsample_bytree'],\n",
    "                           min_child_weight=param_tuned['min_child_weight']).fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "yhat_model_xgb_tuned=model_xgb_tuned.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance\n",
    "model_xgb_result_tuned=ml_error('XGBoost Regressor',np.expm1(y_test),np.expm1(yhat_model_xgb_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Regressor</td>\n",
       "      <td>765.146464</td>\n",
       "      <td>0.116287</td>\n",
       "      <td>1093.511057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model name         MAE      MAPE         RMSE\n",
       "0  XGBoost Regressor  765.146464  0.116287  1093.511057"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('code8_model_xgb_result_tuned.pkl', 'wb') as f:\n",
    "    pickle.dump(model_xgb_result_tuned, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
